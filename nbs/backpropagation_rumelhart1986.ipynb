{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backpropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "* _Rumelhart et al. 1986, Learning representations by back-propagating errors_, [nature](https://www.nature.com/articles/323533a0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import sklearn.datasets as sk_datasets\n",
    "import torch\n",
    "from sklearn import metrics\n",
    "\n",
    "import random_neural_net_models.backprop_rumelhart as backprop_rumelhart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = sk_datasets.make_regression(\n",
    "    n_samples=100, n_features=1, noise=20, random_state=SEED\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(x=X[:, 0], y=y)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = sk_datasets.make_blobs(\n",
    "    n_samples=1_000,\n",
    "    n_features=2,\n",
    "    centers=2,\n",
    "    random_state=SEED,\n",
    ")\n",
    "sns.scatterplot(x=X[:, 0], y=X[:, 1], hue=y, alpha=0.3);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rumelhart et al. 1986, \"Learning representations by back-propagating errors\"\n",
    "> The following is in the spirit of the paper, *cough*. Found the presentation in the paper somewhat hard to read."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With input $x$ we predict $y$ using a function $f$ like\n",
    "\n",
    "$$\n",
    "\n",
    "y = f(x)\n",
    "\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$f$ uses weight $w$ and some non-linearity $g$ like\n",
    "\n",
    "$$\n",
    "z =  x \\cdot w^T \\\\\n",
    "f(x) = g(z)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the paper they use the sigmoid\n",
    "$$\n",
    "g(z) = \\frac{1}{1+\\exp(-z)}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indicating repeating weight multiplication and non-linearity with index $i$ (layer), we can write\n",
    "\n",
    "$$\n",
    "\n",
    "z_1 =  x \\cdot w_1 ^ T \\\\\n",
    "a_1 = g_1(z_1) \\\\\n",
    "\n",
    "...\\\\\n",
    "\n",
    "z_{i} = a_{i-1} \\cdot w_{i}^T \\\\\n",
    "a_{i} = g_{i}(z_{i}) \\\\\n",
    "\n",
    "...\\\\\n",
    "\n",
    "z_N = a_{N-1} \\cdot w_N^T \\\\\n",
    "a_N = g_N(z_N) \\\\\n",
    "\n",
    "f(x) = a_N\n",
    "\n",
    "$$\n",
    "\n",
    "where $a_i$ is \"activation\" of layer $i$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparing predictions to desired valued $d$ we denote deviations / the loss as $l(d,y)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this we can identify how to change $w_i$ by how much lead to the strongest improvement of $l(d,y)$ for any $x$, $y$, $d$ by differentiating $l$. A straightforward way is scaling that gradient and applying it like\n",
    "\n",
    "$$\n",
    "\n",
    "w_{i,\\text{new}} = w_{i,\\text{old}} + \\epsilon \\frac{d}{dw_i}l(d,y)\n",
    "\n",
    "$$\n",
    "\n",
    "with some factor $\\epsilon \\in [0,1]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\frac{d}{dw_i}l(d,y)$ can analytically be derived using the chain rule as \n",
    "\n",
    "$$\n",
    "\n",
    "\\frac{d}{dw_i}l(d,y) = l^\\prime(d,y) \\cdot g_N^\\prime(z_N) \\cdot z_N^\\prime \\cdot g_{N-1}^\\prime(z_{N-1}) \\cdot z_{N-1}^\\prime \\cdot \\space ... \\space \\cdot g_{i}^\\prime(z_{i}) \\cdot a_{i-1}\n",
    "\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Replacing $z^\\prime = \\frac{\\partial}{\\partial a}z = w ^ T$\n",
    "$$\n",
    "\n",
    "\\frac{d}{dw_i}l(d,y) = l^\\prime(d,y) \\cdot g_N^\\prime(z_N) \\cdot w_N ^ T \\cdot g_{N-1}^\\prime(z_{N-1}) \\cdot w_{N-1} ^ T \\cdot \\space ... \\space \\cdot g_{i}^\\prime(z_{i}) \\cdot a_{i-1}\n",
    "\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another version proposed in Rumelhart et al. is\n",
    "$$\n",
    "\n",
    "w_{i,\\text{new}} = w_{i,\\text{old}} + \\epsilon \\frac{d}{dw_i}l(d,y) + \\alpha \\left( \\frac{d}{dw_i}l(d,y) \\right)_\\text{old}\n",
    "\n",
    "$$\n",
    "\n",
    "with some factor $\\alpha \\in [0,1]$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.linspace(-10, 10, 100)\n",
    "a = backprop_rumelhart.sigmoid(x)\n",
    "a_prime = backprop_rumelhart.sigmoid_derivative(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lineplot(x=x, y=a, label=\"g(z): sigmoid\")\n",
    "sns.lineplot(x=x, y=a_prime, label=\"dg(z)/dz: sigmoid derivative\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = backprop_rumelhart.Rumelhart1986PerceptronClassifier(\n",
    "    n_hidden=(10, 5), epochs=10, verbose=True, eps=1e-3, alpha=1e-3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.errors_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x0 = np.linspace(X[:, 0].min(), X[:, 0].max(), 100)\n",
    "x1 = np.linspace(X[:, 1].min(), X[:, 1].max(), 100)\n",
    "X0, X1 = np.meshgrid(x0, x1)\n",
    "X_plot = np.array([X0.ravel(), X1.ravel()]).T\n",
    "X_plot[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_prob = model.predict_proba(X_plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "im = ax.pcolormesh(X0, X1, y_prob.reshape(X0.shape), alpha=0.2)\n",
    "fig.colorbar(im, ax=ax)\n",
    "sns.scatterplot(x=X[:, 0], y=X[:, 1], hue=y, ax=ax, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(metrics.classification_report(y, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
