{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dense Variational Autoencoder on MNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "* fastai 2022 / 2023 course part II:\n",
    "    * [notebook 29](https://github.com/fastai/course22p2/blob/master/nbs/29_vae.ipynb)\n",
    "    * [lesson 25](https://course.fast.ai/Lessons/lesson25.html)\n",
    "* https://github.com/sksq96/pytorch-vae"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import re\n",
    "import typing as T\n",
    "from collections import defaultdict\n",
    "from functools import partial\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import tqdm\n",
    "from einops import rearrange\n",
    "from einops.layers.torch import Rearrange\n",
    "from sklearn import metrics\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.optim import SGD, Adam\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import random_neural_net_models.cnn_autoencoder_fastai2022 as cnn_ae\n",
    "import random_neural_net_models.convolution_lecun1990 as conv_lecun1990\n",
    "\n",
    "sns.set_theme()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist = fetch_openml(\"mnist_784\", version=1, cache=True, parser=\"auto\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting seeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "\n",
    "random.seed(42)\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_device() -> str:\n",
    "    return \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "\n",
    "device = get_device()\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = mnist[\"data\"]\n",
    "y = mnist[\"target\"]\n",
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Selecting a few images to overfit on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n0 = 10\n",
    "n1 = 1_000\n",
    "X0, y0 = X.iloc[:n0], y.iloc[:n0]\n",
    "X1, y1 = X.iloc[n0 : n1 + n0], y.iloc[n0 : n0 + n1]\n",
    "X0.shape, X1.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining dataset and dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = conv_lecun1990.DigitsDataset(X0, y0)\n",
    "ds_test = conv_lecun1990.DigitsDataset(X1, y1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item = ds[0]\n",
    "plt.imshow(item[0], cmap=\"gray\", origin=\"upper\")\n",
    "plt.title(f\"Label: {item[1]}\")\n",
    "plt.axis(\"off\")\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "defining a dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 10\n",
    "dataloader = DataLoader(ds, batch_size=batch_size, shuffle=False)\n",
    "dataloader_test = DataLoader(ds_test, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    # https://github.com/sksq96/pytorch-vae/blob/master/vae.py\n",
    "    # https://github.com/fastai/course22p2/blob/master/nbs/29_vae.ipynb\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        # ks = 3\n",
    "        # stride = 2\n",
    "        # padding = ks // 2\n",
    "\n",
    "        self.flatten_input = Rearrange(\"b h w -> b (h w)\")\n",
    "        # self.add_padding = nn.ZeroPad2d(2)\n",
    "        # self.enc_conv1 = nn.Conv2d(\n",
    "        #     1, 2, kernel_size=ks, stride=stride, padding=padding\n",
    "        # )\n",
    "        h, w = 28, 28\n",
    "        n_input = h * w\n",
    "        n_hidden = 400\n",
    "        n_latent = 200\n",
    "        self.enc_dense1 = nn.Linear(n_input, n_hidden)\n",
    "        self.enc_act1 = nn.ReLU()\n",
    "        # self.enc_act1 = nn.SiLU()\n",
    "        self.enc_bn1 = nn.BatchNorm1d(n_hidden)\n",
    "        # self.enc_conv2 = nn.Conv2d(\n",
    "        #     2, 4, kernel_size=ks, stride=stride, padding=padding\n",
    "        # )\n",
    "        self.enc_dense2 = nn.Linear(n_hidden, n_hidden)\n",
    "        self.enc_act2 = nn.ReLU()\n",
    "        # self.enc_act2 = nn.SiLU()\n",
    "        self.enc_bn2 = nn.BatchNorm1d(n_hidden)\n",
    "\n",
    "        nn.init.kaiming_normal_(self.enc_dense1.weight)\n",
    "        nn.init.kaiming_normal_(self.enc_dense2.weight)\n",
    "\n",
    "        self.encoder = nn.Sequential(\n",
    "            self.flatten_input,  # 28x28 -> 784\n",
    "            self.enc_dense1,  # 784 -> 400\n",
    "            self.enc_act1,\n",
    "            self.enc_bn1,\n",
    "            self.enc_dense2,  # 400 -> 200\n",
    "            self.enc_act2,\n",
    "            self.enc_bn2,\n",
    "        )\n",
    "\n",
    "        # variational / latent part\n",
    "        # n_conv2 = 4 * 8 * 8\n",
    "        # n_latent = n_conv2  # 200\n",
    "        # self.conv2flat = Rearrange(\"b c h w -> b (c h w)\")\n",
    "        self.mu = nn.Linear(n_hidden, n_latent)\n",
    "        self.logvar = nn.Linear(n_hidden, n_latent)\n",
    "        self.mu_bn = nn.BatchNorm1d(n_latent)\n",
    "        self.logvar_bn = nn.BatchNorm1d(n_latent)\n",
    "\n",
    "        nn.init.kaiming_normal_(self.logvar.weight)\n",
    "        nn.init.kaiming_normal_(self.mu.weight)\n",
    "\n",
    "        self.dec_dense1 = nn.Linear(n_latent, n_hidden)\n",
    "        self.dec_act1 = nn.ReLU()\n",
    "        # self.dec_act1 = nn.SiLU()\n",
    "        self.dec_bn1 = nn.BatchNorm1d(n_hidden)\n",
    "\n",
    "        self.dec_dense2 = nn.Linear(n_hidden, n_hidden)\n",
    "        self.dec_act2 = nn.ReLU()\n",
    "        # self.dec_act2 = nn.SiLU()\n",
    "        self.dec_bn2 = nn.BatchNorm1d(n_hidden)\n",
    "\n",
    "        self.dec_dense3 = nn.Linear(n_hidden, n_input)\n",
    "        # self.dec_act3 = nn.Sigmoid()\n",
    "        self.dec_bn3 = nn.BatchNorm1d(n_input)\n",
    "\n",
    "        nn.init.kaiming_normal_(self.dec_dense1.weight)\n",
    "        nn.init.kaiming_normal_(self.dec_dense2.weight)\n",
    "        nn.init.kaiming_normal_(self.dec_dense3.weight)\n",
    "\n",
    "        self.unflatten_output = Rearrange(\"b (h w) -> b h w\", h=h, w=h)\n",
    "\n",
    "        self.decoder = nn.Sequential(\n",
    "            self.dec_dense1,  # 200 -> 400\n",
    "            self.dec_act1,\n",
    "            self.dec_bn1,\n",
    "            self.dec_dense2,  # 400 -> 400\n",
    "            self.dec_act2,\n",
    "            self.dec_bn2,\n",
    "            self.dec_dense3,  # 400 -> 784\n",
    "            # self.dec_act3,\n",
    "            self.dec_bn3,\n",
    "            self.unflatten_output,  # 784 -> 28x28\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # encode\n",
    "        x = self.encoder(x)\n",
    "\n",
    "        # variational / latent part\n",
    "        mu = self.mu(x)\n",
    "        logvar = self.logvar(x)\n",
    "        mu = self.mu_bn(mu)\n",
    "        logvar = self.logvar_bn(logvar)\n",
    "        std = (0.5 * logvar).exp()\n",
    "        eps = torch.randn_like(std)\n",
    "        z = mu + eps * std\n",
    "\n",
    "        # decode\n",
    "        x_hat = self.decoder(z)\n",
    "\n",
    "        return x_hat, mu, logvar\n",
    "\n",
    "\n",
    "def calc_distribution_divergence_loss(\n",
    "    input: T.Tuple[torch.Tensor, torch.Tensor, torch.Tensor], x: torch.Tensor\n",
    ") -> torch.Tensor:\n",
    "    _, mu, logvar = input\n",
    "    s = 1 + logvar - mu.pow(2) - logvar.exp()\n",
    "    return -0.5 * s.mean()\n",
    "\n",
    "\n",
    "def calc_reconstruction_loss(\n",
    "    input: T.Tuple[torch.Tensor, torch.Tensor, torch.Tensor], x: torch.Tensor\n",
    ") -> torch.Tensor:\n",
    "    x_hat, _, _ = input\n",
    "    # return F.mse_loss(x, x_hat)\n",
    "    return F.binary_cross_entropy_with_logits(x_hat, x)\n",
    "\n",
    "\n",
    "def calc_vae_loss(\n",
    "    input: T.Tuple[torch.Tensor, torch.Tensor, torch.Tensor], x: torch.Tensor\n",
    ") -> T.Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "    reconstruction_loss = calc_reconstruction_loss(input, x)\n",
    "    divergence_loss = calc_distribution_divergence_loss(input, x)\n",
    "    total_loss = reconstruction_loss + divergence_loss\n",
    "    return total_loss, reconstruction_loss, divergence_loss\n",
    "\n",
    "\n",
    "def calc_vae_test_loss(\n",
    "    model_output: T.List[T.Tuple[torch.Tensor, torch.Tensor, torch.Tensor]],\n",
    "    x: torch.Tensor,\n",
    ") -> T.Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "    x_hat = torch.cat([_x[0] for _x in model_output], dim=0)\n",
    "    mu = torch.cat([_x[1] for _x in model_output], dim=0)\n",
    "    logvar = torch.cat([_x[2] for _x in model_output], dim=0)\n",
    "    _model_output = (x_hat, mu, logvar)\n",
    "    reconstruction_loss = calc_reconstruction_loss(_model_output, x)\n",
    "    divergence_loss = calc_distribution_divergence_loss(_model_output, x)\n",
    "    total_loss = reconstruction_loss + divergence_loss\n",
    "    return total_loss, reconstruction_loss, divergence_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_module_name_is_activation(name: str) -> bool:\n",
    "    return re.match(r\".*act\\d$\", name) is not None\n",
    "\n",
    "\n",
    "print(\n",
    "    check_module_name_is_activation(\"act1\"),\n",
    "    check_module_name_is_activation(\"blub_act1\"),\n",
    "    check_module_name_is_activation(\"blub\"),\n",
    "    check_module_name_is_activation(\"act1_bla\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_module_name_grad_relevant(name: str) -> bool:\n",
    "    return (\n",
    "        name not in [\"unflatten_output\", \"encoder\", \"decoder\"]\n",
    "    ) and re.match(r\".*act\\d$\", name) is None\n",
    "\n",
    "\n",
    "print(\n",
    "    check_module_name_grad_relevant(\"unflatten_output\"),\n",
    "    check_module_name_grad_relevant(\"encoder\"),\n",
    "    check_module_name_grad_relevant(\"decoder\"),\n",
    "    check_module_name_grad_relevant(\"dec_bn3\"),\n",
    "    check_module_name_grad_relevant(\"dec_act3\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model()\n",
    "model = conv_lecun1990.ModelTelemetry(\n",
    "    model,\n",
    "    func_is_act=check_module_name_is_activation,\n",
    "    func_is_grad_relevant=check_module_name_grad_relevant,\n",
    "    loss_names=(\"total\", \"reconstruction\", \"divergence\"),\n",
    ")\n",
    "model.double()\n",
    "model.to(device);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# opt = SGD(\n",
    "#     model.parameters(),\n",
    "#     lr=0.1,\n",
    "# )\n",
    "opt = Adam(model.parameters(), lr=3e-2, eps=1e-5)\n",
    "opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_func = calc_vae_loss\n",
    "loss_func_test = calc_vae_test_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 100\n",
    "_iter = 0\n",
    "\n",
    "model.train()\n",
    "for epoch in tqdm.tqdm(range(n_epochs), desc=\"Epochs\", total=n_epochs):\n",
    "    for i, (xb, _) in enumerate(dataloader):\n",
    "        xb = xb.to(device)\n",
    "        x_pred = model(xb)\n",
    "\n",
    "        loss, reconstruction_loss, divergence_loss = loss_func(x_pred, xb)\n",
    "\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "\n",
    "        model.loss_history_train(\n",
    "            (loss, reconstruction_loss, divergence_loss), _iter\n",
    "        )\n",
    "        model.parameter_history(_iter)\n",
    "\n",
    "        _iter += 1\n",
    "\n",
    "    # compute validation loss\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        xs_pred, xs_true = [], []\n",
    "        for xb, _ in dataloader_test:\n",
    "            xb = xb.to(device)\n",
    "\n",
    "            x_pred = model(xb)\n",
    "            xs_pred.append(x_pred)\n",
    "            xs_true.append(xb)\n",
    "\n",
    "        x_true = torch.cat(xs_true, dim=0)\n",
    "        (\n",
    "            loss_test,\n",
    "            reconstruction_loss_test,\n",
    "            divergence_loss_test,\n",
    "        ) = loss_func_test(xs_pred, x_true)\n",
    "\n",
    "        model.loss_history_test(\n",
    "            (loss_test, reconstruction_loss_test, divergence_loss_test), _iter\n",
    "        )\n",
    "\n",
    "        model.train()\n",
    "\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "plotting gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.draw_gradient_stats(yscale=\"log\", figsize=(12, 20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "plotting activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.draw_activation_stats(yscale=\"log\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "plotting losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.draw_loss_history_train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.draw_loss_history_test(yscale=\"log\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "drawing histograms of the weights and biases across training iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.draw_parameter_stats(\n",
    "    \"enc_dense1\",\n",
    "    \"enc_dense2\",\n",
    "    \"mu\",\n",
    "    \"logvar\",\n",
    "    \"dec_dense1\",\n",
    "    \"dec_dense2\",\n",
    "    \"dec_dense3\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.clean_hooks()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features, _ = next(iter(dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "inspecting predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features = train_features.to(device)\n",
    "preds, _, _ = model(train_features)\n",
    "preds[0, :5, :5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_pred = preds.detach().sigmoid().cpu().numpy()\n",
    "x_pred[0, :3, :5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = train_features[0].cpu()\n",
    "img_pred = x_pred[0]\n",
    "fig, axs = plt.subplots(nrows=1, ncols=2, figsize=(10, 5))\n",
    "ax = axs[0]\n",
    "ax.imshow(img, cmap=\"gray\")\n",
    "ax.set_title(\"Input image\")\n",
    "ax.axis(\"off\")\n",
    "ax = axs[1]\n",
    "ax.imshow(img_pred, cmap=\"gray\")\n",
    "ax.set_title(\"Reconstructed image\")\n",
    "ax.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we can overfit using this setup. Interestingly there seem to be 3 stages of optimization and it took about 15k iterations to get there and there still seems to be some room. So more iterations than without the variational / latent component. Other notable differences to the plain autoencoder are:\n",
    "* overfitting not achieved within 10k iterations if the `mu` and `logvar` estimates are not fed into a dense layer before reshaping back into 8x8x4 for deconvolution\n",
    "* the loss is much noisier with the variational approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reproducing 10 digits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X0, X1, y0, y1 = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = conv_lecun1990.DigitsDataset(X0, y0)\n",
    "ds_test = conv_lecun1990.DigitsDataset(X1, y1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 256\n",
    "dataloader = DataLoader(ds, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "dataloader_test = DataLoader(\n",
    "    ds_test, batch_size=500, shuffle=False, drop_last=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model()\n",
    "model = conv_lecun1990.ModelTelemetry(\n",
    "    model,\n",
    "    func_is_act=check_module_name_is_activation,\n",
    "    func_is_grad_relevant=check_module_name_grad_relevant,\n",
    "    loss_names=(\"total\", \"reconstruction\", \"divergence\"),\n",
    "    parameter_every_n=100,\n",
    "    activations_every_n=100,\n",
    "    gradients_every_n=100,\n",
    ")\n",
    "model.double()\n",
    "model.to(device);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# opt = SGD(\n",
    "#     model.parameters(),\n",
    "#     lr=0.1,\n",
    "# )\n",
    "opt = Adam(model.parameters(), lr=3e-2, eps=1e-5)\n",
    "opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss_history = conv_lecun1990.LossHistory(every_n=1)\n",
    "# loss_history_reconstruction = conv_lecun1990.LossHistory(every_n=1)\n",
    "# divergence_loss_history = conv_lecun1990.LossHistory(every_n=1)\n",
    "\n",
    "# loss_history_test = conv_lecun1990.LossHistory(every_n=1)\n",
    "# loss_history_reconstruction_test = conv_lecun1990.LossHistory(every_n=1)\n",
    "# loss_history_divergence_test = conv_lecun1990.LossHistory(every_n=1)\n",
    "\n",
    "# parameter_history = ParameterHistory(every_n=1)\n",
    "# hooks = get_hooks(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_iter = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 10\n",
    "\n",
    "model.train()\n",
    "for epoch in tqdm.tqdm(range(n_epochs), desc=\"Epochs\", total=n_epochs):\n",
    "    for xb, _ in dataloader:\n",
    "        xb = xb.to(device)\n",
    "        x_pred = model(xb)\n",
    "\n",
    "        loss, reconstruction_loss, divergence_loss = loss_func(x_pred, xb)\n",
    "\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "\n",
    "        model.loss_history_train(\n",
    "            (loss, reconstruction_loss, divergence_loss), _iter\n",
    "        )\n",
    "        model.parameter_history(_iter)\n",
    "\n",
    "        # parameter_history(model, _iter)\n",
    "        # loss_history(loss, _iter)\n",
    "        # loss_history_reconstruction(reconstruction_loss, _iter)\n",
    "        # divergence_loss_history(divergence_loss, _iter)\n",
    "\n",
    "        _iter += 1\n",
    "\n",
    "    # compute validation loss\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        xs_pred, xs_true = [], []\n",
    "        for xb, _ in dataloader_test:\n",
    "            xb = xb.to(device)\n",
    "\n",
    "            x_pred = model(xb)\n",
    "            xs_pred.append(x_pred)\n",
    "            xs_true.append(xb)\n",
    "\n",
    "        x_true = torch.cat(xs_true, dim=0)\n",
    "        (\n",
    "            loss_test,\n",
    "            reconstruction_loss_test,\n",
    "            divergence_loss_test,\n",
    "        ) = loss_func_test(xs_pred, x_true)\n",
    "\n",
    "        model.loss_history_test(\n",
    "            (loss_test, reconstruction_loss_test, divergence_loss_test), _iter\n",
    "        )\n",
    "\n",
    "        # loss_history_test(loss_test, _iter)\n",
    "        # loss_history_reconstruction_test(reconstruction_loss_test, _iter)\n",
    "        # loss_history_divergence_test(divergence_loss_test, _iter)\n",
    "        model.train()\n",
    "\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "plotting gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.draw_gradient_stats(yscale=\"log\", figsize=(12, 20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "plotting activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.draw_activation_stats(yscale=\"log\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "plotting losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.draw_loss_history_train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.draw_loss_history_test(yscale=\"log\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "drawing histograms of the weights and biases across training iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.draw_parameter_stats(\n",
    "    \"enc_dense1\",\n",
    "    \"enc_dense2\",\n",
    "    \"mu\",\n",
    "    \"logvar\",\n",
    "    \"dec_dense1\",\n",
    "    \"dec_dense2\",\n",
    "    \"dec_dense3\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.clean_hooks()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: why are there spikes in batchnorm std?\n",
    "# TODO: why do quantile values for some layers disappear / appear for 5% and 50%?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: enc_act1 and enc_act2 are pretty much 0, why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_features, _ = next(iter(dataloader_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "inspecting predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_features = test_features.to(device)\n",
    "preds, _, _ = model(test_features)\n",
    "preds[0, :5, :5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_features[0, :3, :5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_pred = preds.detach().sigmoid().cpu().numpy()\n",
    "x_pred[0, :3, :5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_pair(img: torch.Tensor, img_pred: torch.Tensor):\n",
    "    fig, axs = plt.subplots(nrows=1, ncols=2, figsize=(10, 5))\n",
    "    ax = axs[0]\n",
    "    ax.imshow(img, cmap=\"gray\")\n",
    "    ax.set_title(\"Input image\")\n",
    "    ax.axis(\"off\")\n",
    "    ax = axs[1]\n",
    "    ax.imshow(img_pred, cmap=\"gray\")\n",
    "    ax.set_title(\"Reconstructed image\")\n",
    "    ax.axis(\"off\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def draw_n_pairs(\n",
    "    input_features: torch.Tensor, x_pred: torch.Tensor, n: int = 5\n",
    "):\n",
    "    _n = min(n, len(input_features))\n",
    "    print(f\"Drawing {_n} pairs\")\n",
    "    for i in range(_n):\n",
    "        img = input_features[i].cpu()\n",
    "        img_pred = x_pred[i]\n",
    "        draw_pair(img, img_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_n_pairs(test_features, x_pred, n=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ":O so apparently batchnorm helps to prevent the dying-off of activations of the second encoding layer, leading to infer always grey blobs. also adam optimizer helps fitting the batchnorm model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
