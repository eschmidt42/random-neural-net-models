{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN Autoencoder on mnist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "* fastai 2022 / 2023 course part II:\n",
    "    * [notebook 29](https://github.com/fastai/course22p2/blob/master/nbs/29_vae.ipynb)\n",
    "    * [lesson 25](https://course.fast.ai/Lessons/lesson25.html)\n",
    "* https://github.com/sksq96/pytorch-vae"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import typing as T\n",
    "from collections import defaultdict\n",
    "from functools import partial\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import tqdm\n",
    "from einops import rearrange\n",
    "from einops.layers.torch import Rearrange\n",
    "from sklearn import metrics\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.optim import SGD\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "import random_neural_net_models.cnn_autoencoder_fastai2022 as cnn_ae\n",
    "import random_neural_net_models.convolution_lecun1990 as conv_lecun1990\n",
    "\n",
    "sns.set_theme()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist = fetch_openml(\"mnist_784\", version=1, cache=True, parser=\"auto\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting seeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "\n",
    "random.seed(42)\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_device() -> str:\n",
    "    return \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "\n",
    "device = get_device()\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = mnist[\"data\"]\n",
    "y = mnist[\"target\"]\n",
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Selecting a few images to overfit on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 1\n",
    "X0, y0 = X.iloc[:n], y.iloc[:n]\n",
    "X0.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining dataset and dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = conv_lecun1990.DigitsDataset(X0, y0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item = ds[0]\n",
    "plt.imshow(item[0], cmap=\"gray\", origin=\"upper\")\n",
    "plt.title(f\"Label: {item[1]}\")\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "defining a dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1\n",
    "dataloader = DataLoader(ds, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    # https://github.com/sksq96/pytorch-vae/blob/master/vae.py\n",
    "    # https://github.com/fastai/course22p2/blob/master/nbs/29_vae.ipynb\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        ks = 3\n",
    "        stride = 2\n",
    "        padding = ks // 2\n",
    "\n",
    "        self.add_dim = Rearrange(\"b h w -> b 1 h w\")\n",
    "        self.add_padding = nn.ZeroPad2d(2)\n",
    "        self.enc_conv1 = nn.Conv2d(\n",
    "            1, 2, kernel_size=ks, stride=stride, padding=padding\n",
    "        )\n",
    "        self.enc_act1 = nn.ReLU()\n",
    "        self.enc_conv2 = nn.Conv2d(\n",
    "            2, 4, kernel_size=ks, stride=stride, padding=padding\n",
    "        )\n",
    "        self.enc_act2 = nn.ReLU()\n",
    "\n",
    "        self.encoder = nn.Sequential(\n",
    "            self.add_dim,  # 28x28 -> 1x28x28\n",
    "            self.add_padding,  # 1x28x28 -> 1x32x32\n",
    "            self.enc_conv1,  # 1x32x32 -> 1x16x16x2\n",
    "            self.enc_act1,\n",
    "            self.enc_conv2,  # 1x16x16x2 -> 1x8x8x4\n",
    "            self.enc_act2,\n",
    "        )\n",
    "\n",
    "        # variational / latent part\n",
    "        n_conv2 = 4 * 8 * 8\n",
    "        n_latent = n_conv2  # 200\n",
    "        self.conv2flat = Rearrange(\"b c h w -> b (c h w)\")\n",
    "        self.mu = nn.Linear(n_conv2, n_latent)\n",
    "        self.logvar = nn.Linear(n_conv2, n_latent)\n",
    "        self.latent2conv = nn.Linear(n_latent, n_conv2)\n",
    "        self.flat2conv = Rearrange(\"b (c h w) -> b c h w\", c=4, h=8, w=8)\n",
    "\n",
    "        self.dec_deconv1 = cnn_ae.DeConv2d(4, 2, kernel_size=ks, stride=1)\n",
    "        self.dec_act1 = nn.ReLU()\n",
    "        self.dec_deconv2 = cnn_ae.DeConv2d(2, 1, kernel_size=ks, stride=1)\n",
    "        self.dec_act2 = nn.Sigmoid()\n",
    "        self.rm_padding = nn.ZeroPad2d(-2)\n",
    "        self.rm_dim = Rearrange(\"b 1 h w -> b h w\")\n",
    "\n",
    "        self.decoder = nn.Sequential(\n",
    "            self.dec_deconv1,  # 1x8x8x4 -> 1x16x16x2\n",
    "            self.dec_act1,\n",
    "            self.dec_deconv2,  # 1x16x16x2 -> 1x32x32\n",
    "            self.rm_padding,  # 1x32x32 -> 1x28x28\n",
    "            self.dec_act2,\n",
    "            self.rm_dim,  # 1x28x28 -> 28x28\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # encode\n",
    "        x = self.encoder(x)\n",
    "\n",
    "        # projecting from 1x8x8x4 to 1x256\n",
    "        x = self.conv2flat(x)\n",
    "\n",
    "        # variational / latent part\n",
    "        mu = self.mu(x)\n",
    "        logvar = self.logvar(x)\n",
    "        std = (0.5 * logvar).exp()\n",
    "        eps = torch.randn_like(std)\n",
    "        x = mu + eps * std\n",
    "\n",
    "        # projecting back from 1xn_latent to 1x8x8x4\n",
    "        x = self.latent2conv(x)\n",
    "        x = self.flat2conv(x)\n",
    "\n",
    "        # decode\n",
    "        x = self.decoder(x)\n",
    "\n",
    "        return x, mu, logvar\n",
    "\n",
    "\n",
    "def calc_distribution_divergence_loss(\n",
    "    input: T.Tuple[torch.Tensor, torch.Tensor, torch.Tensor], x: torch.Tensor\n",
    ") -> torch.Tensor:\n",
    "    _, mu, logvar = input\n",
    "    s = 1 + logvar - mu.pow(2) - logvar.exp()\n",
    "    return -0.5 * s.mean()\n",
    "\n",
    "\n",
    "def calc_reconstruction_loss(\n",
    "    input: T.Tuple[torch.Tensor, torch.Tensor, torch.Tensor], x: torch.Tensor\n",
    ") -> torch.Tensor:\n",
    "    x_hat, _, _ = input\n",
    "    return F.mse_loss(x, x_hat)\n",
    "\n",
    "\n",
    "def calc_vae_loss(\n",
    "    input: T.Tuple[torch.Tensor, torch.Tensor, torch.Tensor], x: torch.Tensor\n",
    ") -> T.Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "    reconstruction_loss = calc_reconstruction_loss(input, x)\n",
    "    divergence_loss = calc_distribution_divergence_loss(input, x)\n",
    "    total_loss = reconstruction_loss + divergence_loss\n",
    "    return total_loss, reconstruction_loss, divergence_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model()\n",
    "model.double()\n",
    "model.to(device);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = SGD(\n",
    "    model.parameters(),\n",
    "    lr=0.1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_func = calc_vae_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hooks(\n",
    "    model: cnn_ae.Model,\n",
    "    hook_func: T.Callable = partial(\n",
    "        conv_lecun1990.append_stats, hist_range=(0, 4)\n",
    "    ),\n",
    ") -> T.List[conv_lecun1990.Hook]:\n",
    "    model_acts = [\n",
    "        model.enc_act1,\n",
    "        model.enc_act2,\n",
    "        model.dec_act1,\n",
    "    ]\n",
    "    act_names = [\"enc_act1\", \"enc_act2\", \"dec_act1\"]\n",
    "    hooks = [\n",
    "        conv_lecun1990.Hook(layer, hook_func, name=name)\n",
    "        for name, layer in zip(act_names, model_acts)\n",
    "    ]\n",
    "    return hooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ParameterHistory:\n",
    "    def __init__(\n",
    "        self,\n",
    "        every_n: int = 1,\n",
    "        hist_bins: int = 80,\n",
    "        hist_range: T.Tuple[float, float] = (0.0, 2.0),\n",
    "    ):\n",
    "        self.history = defaultdict(list)\n",
    "        self.every_n = every_n\n",
    "        self.iter = []\n",
    "        self.hist_bins = hist_bins\n",
    "        self.hist_range = hist_range\n",
    "\n",
    "    def __call__(self, model: nn.Module, _iter: int):\n",
    "        if _iter % self.every_n != 0:\n",
    "            return\n",
    "        state_dict = model.state_dict()\n",
    "\n",
    "        for name, tensor in state_dict.items():\n",
    "            counts = (\n",
    "                tensor.clone()\n",
    "                .cpu()\n",
    "                .abs()\n",
    "                .flatten()\n",
    "                .histc(self.hist_bins, self.hist_range[0], self.hist_range[1])\n",
    "                .numpy()\n",
    "            )\n",
    "            self.history[name].append(counts)\n",
    "\n",
    "        self.iter.append(_iter)\n",
    "\n",
    "    def get_df(self, name: str) -> pd.DataFrame:\n",
    "        df = [\n",
    "            pd.DataFrame({\"value\": w}).assign(iter=i)\n",
    "            for i, w in zip(self.iter, self.history[name])\n",
    "        ]\n",
    "        return pd.concat(df, ignore_index=True)[[\"iter\", \"value\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_history = conv_lecun1990.LossHistory(every_n=1)\n",
    "loss_history_reconstruction = conv_lecun1990.LossHistory(every_n=1)\n",
    "divergence_loss_history = conv_lecun1990.LossHistory(every_n=1)\n",
    "parameter_history = ParameterHistory(every_n=1, hist_range=(0, 2))\n",
    "hooks = get_hooks(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 15_000\n",
    "_iter = 0\n",
    "model.train()\n",
    "for epoch in tqdm.tqdm(range(n_epochs), desc=\"Epochs\", total=n_epochs):\n",
    "    for i, (xb, _) in enumerate(dataloader):\n",
    "        xb = xb.to(device)\n",
    "        x_pred = model(xb)\n",
    "\n",
    "        loss, reconstruction_loss, divergence_loss = loss_func(x_pred, xb)\n",
    "\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "\n",
    "        parameter_history(model, _iter)\n",
    "        loss_history(loss, _iter)\n",
    "        loss_history_reconstruction(reconstruction_loss, _iter)\n",
    "        divergence_loss_history(divergence_loss, _iter)\n",
    "\n",
    "        _iter += 1\n",
    "\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "plotting the loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_lecun1990.draw_loss(loss_history, label=\"Train\", window=100)\n",
    "conv_lecun1990.draw_loss(\n",
    "    loss_history_reconstruction, label=\"Train (reconstruction)\", window=100\n",
    ")\n",
    "conv_lecun1990.draw_loss(\n",
    "    divergence_loss_history, label=\"Train (divergence)\", window=100\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "plotting parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stack_weight_history(\n",
    "    history: ParameterHistory, name: str, suffix: str, log1p: bool = True\n",
    ") -> np.ndarray:\n",
    "    hist = np.column_stack(history.history[f\"{name}.{suffix}\"])\n",
    "    if log1p:\n",
    "        hist = np.log1p(hist)\n",
    "    return hist\n",
    "\n",
    "\n",
    "def draw_history(\n",
    "    history: ParameterHistory,\n",
    "    name: str,\n",
    "    figsize: T.Tuple[int, int] = (12, 4),\n",
    "    hist_aspect_w: float = 25.0,\n",
    "    hist_aspect_b: float = 25.0,\n",
    "    log1p: bool = False,\n",
    ") -> None:\n",
    "    fig, axs = plt.subplots(figsize=figsize, nrows=2, sharex=True)\n",
    "\n",
    "    ax = axs[0]\n",
    "\n",
    "    hist = stack_weight_history(\n",
    "        history, name=name, suffix=\"weight\", log1p=log1p\n",
    "    )\n",
    "    ax.imshow(hist, aspect=hist_aspect_w, origin=\"lower\")\n",
    "    ax.set_axis_off()\n",
    "    ax.set_title(f\"{name} - weight\")\n",
    "\n",
    "    ax = axs[1]\n",
    "\n",
    "    hist = stack_weight_history(history, name=name, suffix=\"bias\", log1p=log1p)\n",
    "    ax.imshow(hist, aspect=hist_aspect_b, origin=\"lower\")\n",
    "    ax.set_axis_off()\n",
    "    ax.set_title(f\"{name} - bias\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_history(parameter_history, \"enc_conv1\")\n",
    "draw_history(parameter_history, \"enc_conv2\")\n",
    "draw_history(parameter_history, \"mu\")\n",
    "draw_history(parameter_history, \"logvar\")\n",
    "draw_history(parameter_history, \"latent2conv\")\n",
    "draw_history(parameter_history, \"dec_deconv1\")\n",
    "draw_history(parameter_history, \"dec_deconv2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "plotting activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_lecun1990.draw_activations(hooks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_lecun1990.clear_hooks(hooks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features, _ = next(iter(dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "inspecting predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features = train_features.to(device)\n",
    "preds, _, _ = model(train_features)\n",
    "preds[0, :5, :5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_pred = preds.to(\"cpu\").detach().numpy()\n",
    "x_pred[0, :3, :5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = train_features[0].cpu()\n",
    "img_pred = x_pred[0]\n",
    "fig, axs = plt.subplots(nrows=1, ncols=2, figsize=(10, 5))\n",
    "ax = axs[0]\n",
    "ax.imshow(img, cmap=\"gray\")\n",
    "ax.set_title(\"Input image\")\n",
    "ax.axis(\"off\")\n",
    "ax = axs[1]\n",
    "ax.imshow(img_pred, cmap=\"gray\")\n",
    "ax.set_title(\"Reconstructed image\")\n",
    "ax.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we can overfit using this setup. Interestingly there seem to be 3 stages of optimization and it took about 15k iterations to get there and there still seems to be some room. So more iterations than without the variational / latent component. Other notable differences to the plain autoencoder are:\n",
    "* overfitting not achieved within 10k iterations if the `mu` and `logvar` estimates are not fed into a dense layer before reshaping back into 8x8x4 for deconvolution\n",
    "* the loss is much noisier with the variational approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reproducing 10 digits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X0, X1, y0, y1 = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = conv_lecun1990.DigitsDataset(X0, y0)\n",
    "ds_test = conv_lecun1990.DigitsDataset(X1, y1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 256\n",
    "dataloader = DataLoader(ds, batch_size=batch_size, shuffle=True)\n",
    "dataloader_test = DataLoader(ds_test, batch_size=500, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model()\n",
    "model.double()\n",
    "model.to(device);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = SGD(\n",
    "    model.parameters(),\n",
    "    lr=0.1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_history = conv_lecun1990.LossHistory(every_n=1)\n",
    "loss_history_reconstruction = conv_lecun1990.LossHistory(every_n=1)\n",
    "divergence_loss_history = conv_lecun1990.LossHistory(every_n=1)\n",
    "\n",
    "loss_history_test = conv_lecun1990.LossHistory(every_n=1)\n",
    "loss_history_reconstruction_test = conv_lecun1990.LossHistory(every_n=1)\n",
    "loss_history_divergence_test = conv_lecun1990.LossHistory(every_n=1)\n",
    "\n",
    "parameter_history = ParameterHistory(every_n=1)\n",
    "hooks = get_hooks(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_vae_test_loss(\n",
    "    model_output: T.List[T.Tuple[torch.Tensor, torch.Tensor, torch.Tensor]],\n",
    "    x: torch.Tensor,\n",
    ") -> T.Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "    x_hat = torch.cat([_x[0] for _x in model_output], dim=0)\n",
    "    mu = torch.cat([_x[1] for _x in model_output], dim=0)\n",
    "    logvar = torch.cat([_x[2] for _x in model_output], dim=0)\n",
    "    _model_output = (x_hat, mu, logvar)\n",
    "    reconstruction_loss = calc_reconstruction_loss(_model_output, x)\n",
    "    divergence_loss = calc_distribution_divergence_loss(_model_output, x)\n",
    "    total_loss = reconstruction_loss + divergence_loss\n",
    "    return total_loss, reconstruction_loss, divergence_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_iter = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 100\n",
    "\n",
    "model.train()\n",
    "for epoch in tqdm.tqdm(range(n_epochs), desc=\"Epochs\", total=n_epochs):\n",
    "    for xb, _ in dataloader:\n",
    "        xb = xb.to(device)\n",
    "        x_pred = model(xb)\n",
    "\n",
    "        loss, reconstruction_loss, divergence_loss = loss_func(x_pred, xb)\n",
    "\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "\n",
    "        parameter_history(model, _iter)\n",
    "        loss_history(loss, _iter)\n",
    "        loss_history_reconstruction(reconstruction_loss, _iter)\n",
    "        divergence_loss_history(divergence_loss, _iter)\n",
    "\n",
    "        _iter += 1\n",
    "\n",
    "    # compute validation loss\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        xs_pred, xs_true = [], []\n",
    "        for xb, _ in dataloader_test:\n",
    "            xb = xb.to(device)\n",
    "\n",
    "            x_pred = model(xb)\n",
    "            xs_pred.append(x_pred)\n",
    "            xs_true.append(xb)\n",
    "\n",
    "        x_true = torch.cat(xs_true, dim=0)\n",
    "        (\n",
    "            loss_test,\n",
    "            reconstruction_loss_test,\n",
    "            divergence_loss_test,\n",
    "        ) = calc_vae_test_loss(xs_pred, x_true)\n",
    "\n",
    "        loss_history_test(loss_test, _iter)\n",
    "        loss_history_reconstruction_test(reconstruction_loss_test, _iter)\n",
    "        loss_history_divergence_test(divergence_loss_test, _iter)\n",
    "        model.train()\n",
    "\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "plotting the loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_lecun1990.draw_loss(loss_history)\n",
    "conv_lecun1990.draw_loss(loss_history_reconstruction, label=\"Reconstruction\")\n",
    "conv_lecun1990.draw_loss(divergence_loss_history, label=\"Divergence\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_lecun1990.draw_loss(loss_history_test, label=\"Test\")\n",
    "conv_lecun1990.draw_loss(\n",
    "    loss_history_reconstruction_test, label=\"Test (Reconstruction)\"\n",
    ")\n",
    "conv_lecun1990.draw_loss(\n",
    "    loss_history_divergence_test, label=\"Test (Divergence)\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "plotting parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_history(parameter_history, \"enc_conv1\")\n",
    "draw_history(parameter_history, \"enc_conv2\")\n",
    "draw_history(parameter_history, \"mu\")\n",
    "draw_history(parameter_history, \"logvar\")\n",
    "draw_history(parameter_history, \"latent2conv\")\n",
    "draw_history(parameter_history, \"dec_deconv1\")\n",
    "draw_history(parameter_history, \"dec_deconv2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "plotting activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_lecun1990.draw_activations(hooks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: enc_act1 and enc_act2 are pretty much 0, why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_lecun1990.clear_hooks(hooks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_features, _ = next(iter(dataloader_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "inspecting predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_features = test_features.to(device)\n",
    "preds, _, _ = model(test_features)\n",
    "preds[0, :5, :5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_features[0, :3, :5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_pred = preds.to(\"cpu\").detach().numpy()\n",
    "x_pred[0, :3, :5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_pair(img: torch.Tensor, img_pred: torch.Tensor):\n",
    "    fig, axs = plt.subplots(nrows=1, ncols=2, figsize=(10, 5))\n",
    "    ax = axs[0]\n",
    "    ax.imshow(img, cmap=\"gray\")\n",
    "    ax.set_title(\"Input image\")\n",
    "    ax.axis(\"off\")\n",
    "    ax = axs[1]\n",
    "    ax.imshow(img_pred, cmap=\"gray\")\n",
    "    ax.set_title(\"Reconstructed image\")\n",
    "    ax.axis(\"off\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def draw_n_pairs(\n",
    "    input_features: torch.Tensor, x_pred: torch.Tensor, n: int = 5\n",
    "):\n",
    "    _n = min(n, len(input_features))\n",
    "    print(f\"Drawing {_n} pairs\")\n",
    "    for i in range(_n):\n",
    "        img = input_features[i].cpu()\n",
    "        img_pred = x_pred[i]\n",
    "        draw_pair(img, img_pred)\n",
    "\n",
    "\n",
    "draw_n_pairs(test_features, x_pred, n=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: what is broken that the reconstruction is not working - yields white blobs?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
