{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "following along:\n",
    "* https://colab.research.google.com/drive/1JMLa53HDuA-i7ZBmqV7ZnA3c_fvtXnx-?usp=sharing\n",
    "* https://www.youtube.com/watch?v=zduSFxRajkE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tom lehrer's songs: https://tomlehrersongs.com/\n",
    "\n",
    "the elements song: https://tomlehrersongs.com/wp-content/uploads/2018/12/the-elements.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lyrics = \"\"\"THE ELEMENTS\n",
    "\n",
    "There's antimony, arsenic, aluminum, selenium,\n",
    "And hydrogen and oxygen and nitrogen and rhenium,\n",
    "And nickel, neodymium, neptunium, germanium,\n",
    "And iron, americium, ruthenium, uranium,\n",
    "\n",
    "Europium, zirconium, lutetium, vanadium,\n",
    "And lanthanum and osmium and astatine and radium,\n",
    "And gold and protactinium and indium and gallium,\n",
    "And iodine and thorium and thulium and thallium.\n",
    "\n",
    "There's yttrium, ytterbium, actinium, rubidium,\n",
    "And boron, gadolinium, niobium, iridium,\n",
    "And strontium and silicon and silver and samarium,\n",
    "And bismuth, bromine, lithium, beryllium, and barium.\n",
    "\n",
    "There's holmium and helium and hafnium and erbium,\n",
    "And phosphorus and francium and fluorine and terbium,\n",
    "And manganese and mercury, molybdenum, magnesium,\n",
    "Dysprosium and scandium and cerium and cesium.\n",
    "\n",
    "And lead, praseodymium and platinum, plutonium,\n",
    "Palladium, promethium, potassium, polonium,\n",
    "And tantalum, technetium, titanium, tellurium,\n",
    "And cadmium and calcium and chromium and curium.\n",
    "\n",
    "There's sulfur, californium and fermium, berkelium,\n",
    "And also mendelevium, einsteinium, nobelium,\n",
    "And argon, krypton, neon, radon, xenon, zinc and rhodium,\n",
    "And chlorine, carbon, cobalt, copper, tungsten, tin and sodium.\n",
    "\n",
    "These are the only ones o_f which the news has come to Ha'vard,\n",
    "And there may be many others but they haven't been discavard.\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## basic byte pair encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import abc\n",
    "import string\n",
    "import typing as T\n",
    "from collections import Counter\n",
    "\n",
    "import regex\n",
    "import tqdm\n",
    "\n",
    "import random_neural_net_models.utils as utils\n",
    "\n",
    "tokens = [int(v) for v in lyrics.encode(\"utf-8\")]\n",
    "tokens[:5], tokens[-5:], len(tokens), len(set(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stats(tokens: T.List[int]) -> Counter:\n",
    "    return Counter(zip(tokens[:-1], tokens[1:]))\n",
    "\n",
    "\n",
    "stats = get_stats(tokens)\n",
    "stats.most_common(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge(\n",
    "    tokens: T.List[int],\n",
    "    pair_to_replace: T.Tuple[int, int],\n",
    "    replacement_token: int,\n",
    ") -> T.List[int]:\n",
    "    new_tokens = []\n",
    "    i = 0\n",
    "    while i < len(tokens):\n",
    "        if i < len(tokens) - 1 and tuple(tokens[i : i + 2]) == pair_to_replace:\n",
    "            new_tokens.append(replacement_token)\n",
    "            i += 2\n",
    "        else:\n",
    "            new_tokens.append(tokens[i])\n",
    "            i += 1\n",
    "    return new_tokens\n",
    "\n",
    "\n",
    "merge([5, 6, 6, 7, 9, 1], (6, 7), 99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_symbols = string.ascii_letters + string.digits\n",
    "base_symbols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_tokens = [int(v) for v in base_symbols.encode(\"utf-8\")]\n",
    "base_tokens[:5], base_tokens[-5:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "replacement_token = max(tokens + base_tokens) + 1\n",
    "replacement_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pair_to_replace = stats.most_common()[0][0]\n",
    "pair_to_replace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens2 = merge(tokens, pair_to_replace, replacement_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max(tokens2), max(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(tokens), len(tokens2), len(set(tokens)), len(set(tokens2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log = utils.logger\n",
    "\n",
    "\n",
    "def repeated_merge(\n",
    "    tokens: T.List[int],\n",
    "    vocab_size: int,\n",
    "    show_progress: bool,\n",
    "    base_tokens: T.List[int] = None,\n",
    ") -> T.Tuple[T.List[int], T.Dict[T.Tuple[int, int], int]]:\n",
    "    n0 = len(tokens)\n",
    "    n_used_tokens = len(set(tokens))\n",
    "    n_merges = vocab_size - n_used_tokens\n",
    "    log.info(\n",
    "        f\"repeatedly merging tokens: {n_merges=} to achieve {vocab_size=} with {n_used_tokens=}\"\n",
    "    )\n",
    "\n",
    "    replacement_token = (\n",
    "        max(tokens + base_tokens) if base_tokens else max(tokens)\n",
    "    )\n",
    "    pair_map = {}\n",
    "    for _ in tqdm.tqdm(\n",
    "        range(n_merges), total=n_merges, desc=\"merge\", disable=not show_progress\n",
    "    ):\n",
    "        stats = get_stats(tokens)\n",
    "        pair_to_replace = stats.most_common()[0][0]\n",
    "        replacement_token += 1\n",
    "        tokens = merge(tokens, pair_to_replace, replacement_token)\n",
    "        pair_map[pair_to_replace] = replacement_token\n",
    "    n1 = len(tokens)\n",
    "    log.info(\n",
    "        f\"result: {n0:_d} -> {n1:_d} tokens = compression to {n1/n0:.2%} of tokens\"\n",
    "    )\n",
    "    return tokens, pair_map\n",
    "\n",
    "\n",
    "vocab_size = len(set(tokens)) + 20\n",
    "tokens3, pair_map = repeated_merge(tokens, vocab_size, show_progress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(pair_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(tokens), len(tokens3), len(set(tokens)), len(set(tokens3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pair_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = {idx: bytes([idx]) for idx in set(tokens + base_tokens)}\n",
    "for (token0, token1), idx in pair_map.items():\n",
    "    vocab[idx] = vocab[token0] + vocab[token1]\n",
    "vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode(bpe_tokens: T.List[int], vocab: T.Dict[int, bytes]) -> str:\n",
    "    tokens = [vocab[token] for token in bpe_tokens]\n",
    "    tokens = b\"\".join(tokens)\n",
    "    text = tokens.decode(\"utf-8\", errors=\"replace\")\n",
    "    return text\n",
    "\n",
    "\n",
    "decode(tokens3, vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(\n",
    "    text: str, pair_map: T.Dict[int, T.Tuple[int, int]], show_progress: bool\n",
    ") -> T.List[int]:\n",
    "    tokens = [int(v) for v in text.encode(\"utf-8\")]\n",
    "    log.info(f\"{len(tokens)=:_d}\")\n",
    "    if len(tokens) == 1:\n",
    "        return tokens\n",
    "\n",
    "    for _ in range(len(pair_map)):\n",
    "        stats = get_stats(tokens)\n",
    "        is_done = not any(p in pair_map for p in stats)\n",
    "        if is_done:\n",
    "            return tokens\n",
    "\n",
    "        pair = min(stats, key=lambda pair: pair_map.get(pair, float(\"inf\")))\n",
    "        idx = pair_map[pair]\n",
    "        tokens = merge(tokens, pair, idx)\n",
    "        log.info(f\"{len(tokens)=:_d}\")\n",
    "\n",
    "    return tokens\n",
    "\n",
    "\n",
    "test_bpe_tokens = encode(\"bla bla and bla\", pair_map, show_progress=True)\n",
    "test_bpe_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decode(test_bpe_tokens, vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/openai/gpt-2\n",
    "\n",
    "https://github.com/openai/tiktoken\n",
    "\n",
    "https://github.com/google/sentencepiece"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizer classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_SYMBOLS = string.ascii_letters + string.digits\n",
    "\n",
    "\n",
    "class TokenizerBase(abc.ABC):\n",
    "\n",
    "    base_symbols: str\n",
    "    base_tokens: T.List[int]\n",
    "    vocab: T.Dict[int, bytes]\n",
    "    pair_map: T.Dict[T.Tuple[int, int], int]\n",
    "\n",
    "    def __init__(self, base_symbols: str = None):\n",
    "        self.base_symbols = base_symbols if base_symbols else BASE_SYMBOLS\n",
    "        self.base_tokens = [int(v) for v in self.base_symbols.encode(\"utf-8\")]\n",
    "\n",
    "    @abc.abstractmethod\n",
    "    def fit(self, text: str, vocab_size: int, verbose: int = False): ...\n",
    "\n",
    "    @abc.abstractmethod\n",
    "    def encode(self, text: str) -> T.List[int]: ...\n",
    "\n",
    "    @abc.abstractmethod\n",
    "    def decode(self, tokens: T.List[int]) -> str: ...\n",
    "\n",
    "\n",
    "class TokenizerSimple(TokenizerBase):\n",
    "\n",
    "    def fit(self, text: str, vocab_size: int, verbose: int = False):\n",
    "        tokens = [int(v) for v in text.encode(\"utf-8\", errors=\"replace\")]\n",
    "        _, self.pair_map = repeated_merge(\n",
    "            tokens,\n",
    "            vocab_size,\n",
    "            show_progress=verbose,\n",
    "            base_tokens=self.base_tokens,\n",
    "        )\n",
    "        self.vocab = {\n",
    "            idx: bytes([idx]) for idx in set(tokens + self.base_tokens)\n",
    "        }\n",
    "        for (token0, token1), idx in self.pair_map.items():\n",
    "            self.vocab[idx] = self.vocab[token0] + self.vocab[token1]\n",
    "\n",
    "    def encode(self, text: str) -> T.List[int]:\n",
    "        return encode(text, self.pair_map, show_progress=False)\n",
    "\n",
    "    def decode(self, tokens: T.List[int]) -> str:\n",
    "        return decode(tokens, self.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = TokenizerSimple()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 60\n",
    "tokenizer.fit(lyrics, vocab_size, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phrase = \"From adolescence to senility, bypassing maturity.\"\n",
    "bpe_tokens = tokenizer.encode(phrase)\n",
    "bpe_tokens[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.decode(bpe_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GPT4_SPLIT_PATTERN = r\"\"\"'(?i:[sdmt]|ll|ve|re)|[^\\r\\n\\p{L}\\p{N}]?+\\p{L}+|\\p{N}{1,3}| ?[^\\s\\p{L}\\p{N}]++[\\r\\n]*|\\s*[\\r\\n]|\\s+(?!\\S)|\\s+\"\"\"\n",
    "\n",
    "pattern = regex.compile(GPT4_SPLIT_PATTERN)\n",
    "\n",
    "pattern.findall(phrase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c0 = Counter([1, 1, 1])\n",
    "c1 = Counter([1, 1, 2])\n",
    "c2 = Counter()\n",
    "c2.update(c0)\n",
    "c2.update(c1)\n",
    "c0, c1, c2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.lesswrong.com/posts/aPeJE8bSo6rAFoLqg/solidgoldmagikarp-plus-prompt-generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenizerRegex(TokenizerSimple):\n",
    "\n",
    "    def fit(\n",
    "        self,\n",
    "        text: str,\n",
    "        vocab_size: int,\n",
    "        pattern: regex.Pattern,\n",
    "        verbose: int = False,\n",
    "    ):\n",
    "        self.pattern = pattern\n",
    "        tokens = [\n",
    "            [int(v) for v in chunk.encode(\"utf-8\", errors=\"replace\")]\n",
    "            for chunk in self.pattern.findall(text)\n",
    "        ]\n",
    "        unique_tokens = set(tok for chunk in tokens for tok in chunk)\n",
    "\n",
    "        n_merges = vocab_size - len(unique_tokens)\n",
    "\n",
    "        if n_merges <= 0:\n",
    "            raise ValueError(f\"{n_merges=} needs to be > 0\")\n",
    "\n",
    "        unique_tokens.update(self.base_tokens)\n",
    "        print(unique_tokens)\n",
    "\n",
    "        self.pair_map = {}\n",
    "        # replacement_token = max([tok for chunk in tokens for tok in chunk])\n",
    "        replacement_token = max(unique_tokens)\n",
    "\n",
    "        for i in range(n_merges):\n",
    "\n",
    "            stats = Counter()\n",
    "            for chunk in tokens:\n",
    "                stats.update(get_stats(chunk))\n",
    "\n",
    "            pair_to_replace = stats.most_common()[0][0]\n",
    "            replacement_token += 1\n",
    "\n",
    "            tokens = [\n",
    "                merge(chunk, pair_to_replace, replacement_token)\n",
    "                for chunk in tokens\n",
    "            ]\n",
    "\n",
    "            self.pair_map[pair_to_replace] = replacement_token\n",
    "\n",
    "        self.vocab = {idx: bytes([idx]) for idx in unique_tokens}\n",
    "        for (token0, token1), idx in self.pair_map.items():\n",
    "            self.vocab[idx] = self.vocab[token0] + self.vocab[token1]\n",
    "\n",
    "    def encode(self, text: str) -> T.List[T.List[int]]:\n",
    "        return [\n",
    "            encode(chunk, self.pair_map, show_progress=False)\n",
    "            for chunk in self.pattern.findall(text)\n",
    "        ]\n",
    "\n",
    "    def decode(self, tokens: T.List[T.List[int]]) -> str:\n",
    "        return \"\".join([decode(chunk, self.vocab) for chunk in tokens])\n",
    "\n",
    "\n",
    "tokenizer = TokenizerRegex()\n",
    "tokenizer.fit(lyrics, vocab_size, pattern)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern.findall(phrase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bpe_tokens = tokenizer.encode(phrase)\n",
    "bpe_tokens[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer.vocab\n",
    "# tokenizer.pair_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.decode(bpe_tokens)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
