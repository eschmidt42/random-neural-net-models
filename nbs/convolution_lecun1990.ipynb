{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First Convolution based neural net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "* LeCun et al. 1990, _Handwritten Digit Recognition: Applications of Neural Net Chips and Automatic Learning_, [Neurocomputing](https://link.springer.com/chapter/10.1007/978-3-642-76153-9_35)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import typing as T\n",
    "\n",
    "# plot first item in dataset\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import tqdm\n",
    "\n",
    "# load mnist using scikit-learn\n",
    "from sklearn.datasets import fetch_openml\n",
    "from torch.optim import SGD\n",
    "from torch.utils.data import DataLoader, Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LeCun et al. 1990, \"Handwritten Digit Recognition: Applications of Neural Net Chips and Automatic Learning\"\n",
    "> The following tries to reproduce the original paper. Note that the digits dataset actually used in the paper could not be found and [MNIST 784](https://www.openml.org/search?type=data&status=active&id=554) is used instead"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "specifics in the paper:\n",
    "\n",
    "* neural net\n",
    "    * weight initialization: uniformly at random $\\in [-2.4 / F_i, 2.4 / F_i]$ with $F_i = $ number of inputs of the unit\n",
    "    * \"tanh activation\": $A \\cdot \\tanh (S \\cdot a)$ with $A = 1.716$, $S = 2/3$ and $a = \\text{weights} \\cdot \\text{input}$\n",
    "    * 256 input (16 x 16 pixel images)\n",
    "    * layer #1: \n",
    "        * convolution with 12 5x5-kernels and stride 2 (output: 8 x 8 x 12 = 786 \"units\")\n",
    "        * tanh activation\n",
    "        * $F_i = 25$?\n",
    "    * layer #2: \n",
    "        * convolution with 12 5x5-kernels and stride 2 (output: 4 x 4 x 12 = 192 \"units\")\n",
    "        * tanh activation\n",
    "        * $F_i = 25$?\n",
    "    * layer #3:\n",
    "        * dense with 30 neurons\n",
    "        * tanh activation\n",
    "        * $F_i = 192$\n",
    "    * layer #4:\n",
    "        * dense output layer with 10 neurons\n",
    "        * tanh activation\n",
    "        * $F_i = 30$\n",
    "* target: vector of 10 values either 1 or -1 (so 9x -1 and 1x 1)\n",
    "* loss: mean squared error between prediction and target (paper reached 1.8e-2 on test and 2.5e-3 on train)\n",
    "* error rates: 0.14% on train, 5% on test\n",
    "* training:\n",
    "    * stochastic gradient descent (1 sample per backpropagation)\n",
    "    * samples always in the same order, no shuffeling\n",
    "    * 23 or 30 epochs, paper is ambiguous\n",
    "    * learning rate was set using some not defined 2nd order derivative method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist = fetch_openml(\"mnist_784\", version=1, cache=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_device() -> str:\n",
    "    return \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "\n",
    "device = get_device()\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = mnist[\"data\"]\n",
    "y = mnist[\"target\"]\n",
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ix0 = 100\n",
    "X0, y0 = X[:ix0], y[:ix0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DigitsDataset(Dataset):\n",
    "    def __init__(self, X: pd.DataFrame, y: pd.Series, edge: int = 28):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.edge = edge\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "\n",
    "    def __getitem__(self, idx: int) -> T.Tuple[torch.Tensor, int]:\n",
    "        img = (\n",
    "            torch.from_numpy(self.X.iloc[idx].values / 255.0)  # normalizing\n",
    "            .reshape(self.edge, self.edge)\n",
    "            .double()\n",
    "        )\n",
    "        label = int(self.y.iloc[idx])\n",
    "        return (img, label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = DigitsDataset(X0, y0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item = ds[4]\n",
    "plt.imshow(item[0], cmap=\"gray\", origin=\"upper\")\n",
    "plt.title(f\"Label: {item[1]}\")\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 4\n",
    "dataloader = DataLoader(ds, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features, train_labels = next(iter(dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Feature batch shape: {train_features.size()}\")\n",
    "print(f\"Labels batch shape: {train_labels.size()}\")\n",
    "img = train_features[0]  # .reshape((28,28))\n",
    "label = train_labels[0]\n",
    "plt.imshow(img, cmap=\"gray\")\n",
    "plt.show()\n",
    "print(f\"Label: {label}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_conv_output_dim(input_dim, kernel_size, padding, stride):\n",
    "    return int((input_dim - kernel_size + 2 * padding) / stride + 1)\n",
    "\n",
    "\n",
    "calc_conv_output_dim(28, 5, 2, 2), calc_conv_output_dim(14, 5, 2, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyConv2d(torch.nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        edge: int,\n",
    "        n_in_channels: int = 1,\n",
    "        n_out_channels: int = 1,\n",
    "        kernel_width: int = 5,\n",
    "        kernel_height: int = 5,\n",
    "        stride: int = 1,\n",
    "        padding: int = 0,\n",
    "        dilation: int = 1,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.weight = torch.randn(\n",
    "            n_out_channels, n_in_channels, kernel_width, kernel_height\n",
    "        ).double()\n",
    "        self.bias = torch.randn(n_out_channels).double()\n",
    "        self.unfold = torch.nn.Unfold(\n",
    "            kernel_size=(kernel_height, kernel_width),\n",
    "            dilation=dilation,\n",
    "            padding=padding,\n",
    "            stride=stride,\n",
    "        )\n",
    "        out_h = out_w = calc_conv_output_dim(\n",
    "            edge, kernel_width, padding, stride\n",
    "        )\n",
    "        self.fold = torch.nn.Fold(\n",
    "            output_size=(out_h, out_w),\n",
    "            kernel_size=(1, 1),\n",
    "            dilation=dilation,\n",
    "            padding=0,\n",
    "            stride=1,\n",
    "        )\n",
    "\n",
    "    def forward(self, input: torch.Tensor) -> torch.Tensor:\n",
    "        # inspiration from: https://discuss.pytorch.org/t/make-custom-conv2d-layer-efficient-wrt-speed-and-memory/70175/2\n",
    "        batch_size, in_channels, in_h, in_w = input.shape\n",
    "        out_channels, in_channels_weight, _, _ = self.weight.shape\n",
    "\n",
    "        if in_h != in_w:\n",
    "            raise ValueError(\n",
    "                f\"Input height {in_h} is not equal to width {in_w}\"\n",
    "            )\n",
    "        if in_channels != in_channels_weight:\n",
    "            raise ValueError(\n",
    "                f\"Input channels {in_channels} is not equal to weight input channels {in_channels_weight}\"\n",
    "            )\n",
    "\n",
    "        # (N,C,in_h,in_w) -> (N, C*kh*kw, num_patches)\n",
    "        # N = batch_size, C = in_channels, kh = kernel_height, kw = kernel_width\n",
    "\n",
    "        input_unfolded = self.unfold(input)\n",
    "\n",
    "        # (N, C*kh*kw, num_patches) -> (N, out_channels, num_patches)\n",
    "        input_unfolded = input_unfolded.transpose(\n",
    "            1, 2\n",
    "        )  # (N, num_patches, C*kh*kw)\n",
    "        weight = self.weight.view(\n",
    "            self.weight.size(0), -1\n",
    "        ).T  # (C*kh*kw, out_channels)\n",
    "        output_unfolded = input_unfolded.matmul(weight).transpose(\n",
    "            1, 2\n",
    "        )  # (N, out_channels, num_patches)\n",
    "\n",
    "        output = self.fold(output_unfolded)  # (N, out_channels, out_h, out_w)\n",
    "        if self.bias is not None:\n",
    "            output += self.bias.view(1, -1, 1, 1)\n",
    "\n",
    "        if output.shape[0] != batch_size:\n",
    "            raise ValueError(\n",
    "                f\"Batch size {batch_size} is not equal to output batch size {output.shape[0]}\"\n",
    "            )\n",
    "        if output.shape[1] != out_channels:\n",
    "            raise ValueError(\n",
    "                f\"Output channels {out_channels} is not equal to output channels {output.shape[1]}\"\n",
    "            )\n",
    "\n",
    "        return output\n",
    "\n",
    "\n",
    "kh = kw = 5\n",
    "n_in_channels = 1\n",
    "n_out_channels = 1\n",
    "weight = torch.randn(n_out_channels, n_in_channels, kw, kw).double()\n",
    "bias = torch.randn(n_out_channels).double()\n",
    "print(f\"{weight.shape=}\")\n",
    "train_features, train_labels = next(iter(dataloader))\n",
    "train_features = train_features.unsqueeze(dim=1)\n",
    "print(f\"{train_features.shape=}\")\n",
    "myconv2d = MyConv2d(\n",
    "    edge=28,\n",
    "    n_in_channels=n_in_channels,\n",
    "    n_out_channels=n_out_channels,\n",
    "    kernel_width=kw,\n",
    "    kernel_height=kh,\n",
    "    stride=2,\n",
    "    padding=2,\n",
    "    dilation=1,\n",
    ")\n",
    "conv_features = myconv2d(train_features)\n",
    "print(f\"{conv_features.shape=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = train_labels[0]\n",
    "fig, axs = plt.subplots(nrows=2, ncols=1, figsize=(5, 5))\n",
    "ax = axs[0]\n",
    "img = train_features[0][0]\n",
    "ax.imshow(img, cmap=\"gray\")\n",
    "ax = axs[1]\n",
    "img = conv_features[0][0]\n",
    "ax.imshow(img, cmap=\"gray\")\n",
    "plt.show()\n",
    "print(f\"Label: {label}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, edge: int = 28, n_classes: int = 10):\n",
    "        super().__init__()\n",
    "\n",
    "        # self.conv1 = nn.Conv2d(1, 12, kernel_size=5, stride=2, padding=2)\n",
    "        self.conv1 = MyConv2d(\n",
    "            edge=edge,\n",
    "            n_in_channels=1,\n",
    "            n_out_channels=12,\n",
    "            kernel_width=5,\n",
    "            kernel_height=5,\n",
    "            stride=2,\n",
    "            padding=2,\n",
    "        )\n",
    "        edge = edge // 2  # effect of stride\n",
    "        # self.conv2 = nn.Conv2d(12, 12, kernel_size=5, stride=2, padding=2)\n",
    "        self.conv2 = MyConv2d(\n",
    "            edge=edge,\n",
    "            n_in_channels=12,\n",
    "            n_out_channels=12,\n",
    "            kernel_width=5,\n",
    "            kernel_height=5,\n",
    "            stride=2,\n",
    "            padding=2,\n",
    "        )\n",
    "        edge = edge // 2  # effect of stride\n",
    "        self.lin1 = nn.Linear(edge * edge * 12, 30)\n",
    "        self.lin2 = nn.Linear(30, n_classes)\n",
    "        self.act = F.tanh\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        x = x.unsqueeze(dim=1)\n",
    "        x = self.act(self.conv1(x))\n",
    "        x = self.act(self.conv2(x))\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.act(self.lin1(x))\n",
    "        x = self.lin2(x)\n",
    "        return F.softmax(x, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model()\n",
    "model.double()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = SGD(model.parameters(), lr=0.1, momentum=0.9, weight_decay=5e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_func = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.conv1.weight.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 20\n",
    "model.train()\n",
    "for epoch in tqdm.tqdm(range(n_epochs), desc=\"Epochs\", total=n_epochs):\n",
    "    for i, (xb, yb) in tqdm.tqdm(\n",
    "        enumerate(dataloader), desc=\"Batches\", total=len(dataloader)\n",
    "    ):\n",
    "        xb = xb.to(device)\n",
    "        yb = yb.to(device)\n",
    "        loss = loss_func(model(xb), yb)\n",
    "\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "\n",
    "    print(loss_func(model(xb), yb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features, train_labels = next(iter(dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features = train_features.to(device)\n",
    "pred_probs = model(train_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = pred_probs.to(\"cpu\").detach().numpy().argmax(axis=1)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Feature batch shape: {train_features.size()}\")\n",
    "print(f\"Labels batch shape: {train_labels.size()}\")\n",
    "img = train_features[0].cpu()  # .reshape((28,28))\n",
    "label = train_labels[0]\n",
    "plt.imshow(img, cmap=\"gray\")\n",
    "plt.show()\n",
    "print(f\"Label: {label}, pred: {y_pred[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_filters = model.conv1.weight.shape[0]\n",
    "\n",
    "fig, axs = plt.subplots(\n",
    "    nrows=n_filters // 3, ncols=n_filters // 4, figsize=(12, 12)\n",
    ")\n",
    "with torch.no_grad():\n",
    "    conv_features = model.act(model.conv1(train_features.unsqueeze(1)))\n",
    "    for i, ax in enumerate(axs.flatten()):\n",
    "        ax.imshow(conv_features[0][i], cmap=\"gray\")\n",
    "        ax.axis(\"off\")\n",
    "        ax.set_title(f\"Filter {i+1}\")\n",
    "\n",
    "plt.show()\n",
    "print(f\"Label: {label}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
