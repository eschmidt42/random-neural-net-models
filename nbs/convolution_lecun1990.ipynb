{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LeCun et al. 1990, \"Handwritten Digit Recognition: Applications of Neural Net Chips and Automatic Learning\"\n",
    "> The following tries to reproduce the original paper. Note that the digits dataset actually used in the paper could not be found and [MNIST 784](https://www.openml.org/search?type=data&status=active&id=554) is used instead"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "* LeCun et al. 1990, _Handwritten Digit Recognition: Applications of Neural Net Chips and Automatic Learning_, [Neurocomputing](https://link.springer.com/chapter/10.1007/978-3-642-76153-9_35)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import typing as T\n",
    "from collections import defaultdict\n",
    "\n",
    "# plot first item in dataset\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import tqdm\n",
    "from einops import rearrange\n",
    "from einops.layers.torch import Rearrange\n",
    "from sklearn import metrics\n",
    "\n",
    "# load mnist using scikit-learn\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.optim import SGD\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "sns.set_theme()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "specifics in the paper:\n",
    "\n",
    "* neural net\n",
    "    * weight initialization: uniformly at random $\\in [-2.4 / F_i, 2.4 / F_i]$ with $F_i = $ number of inputs of the unit\n",
    "    * \"tanh activation\": $A \\cdot \\tanh (S \\cdot a)$ with $A = 1.716$, $S = 2/3$ and $a = \\text{weights} \\cdot \\text{input}$\n",
    "    * 256 input (16 x 16 pixel images)\n",
    "    * layer #1: \n",
    "        * convolution with 12 5x5-kernels and stride 2 (output: 8 x 8 x 12 = 786 \"units\")\n",
    "        * tanh activation\n",
    "        * $F_i = 5 \\cdot 5 \\cdot n_\\text{input-channels} = 5 \\cdot 5 \\cdot 1 = 25$\n",
    "    * layer #2: \n",
    "        * convolution with 12 5x5-kernels and stride 2 (output: 4 x 4 x 12 = 192 \"units\")\n",
    "        * tanh activation\n",
    "        * $F_i = 5 \\cdot 5 \\cdot n_\\text{input-channels} = 5 \\cdot 5 \\cdot 12 = 300$\n",
    "    * layer #3:\n",
    "        * dense with 30 neurons\n",
    "        * tanh activation\n",
    "        * $F_i = 4 \\cdot 4 \\cdot 12 = 192$\n",
    "    * layer #4:\n",
    "        * dense output layer with 10 neurons\n",
    "        * tanh activation\n",
    "        * $F_i = 30$\n",
    "* target: vector of 10 values either 1 or -1 (so 9x -1 and 1x 1)\n",
    "* loss: mean squared error between prediction and target (paper reached 1.8e-2 on test and 2.5e-3 on train)\n",
    "* error rates: 0.14% on train, 5% on test\n",
    "* training:\n",
    "    * stochastic gradient descent (1 sample per backpropagation)\n",
    "    * samples always in the same order, no shuffling\n",
    "    * 23 or 30 epochs, paper is ambiguous\n",
    "    * learning rate was set using some not defined 2nd order derivative method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist = fetch_openml(\"mnist_784\", version=1, cache=True, parser=\"auto\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting seeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "\n",
    "random.seed(42)\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_device() -> str:\n",
    "    return \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "\n",
    "device = get_device()\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = mnist[\"data\"]\n",
    "y = mnist[\"target\"]\n",
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 100\n",
    "X0, y0 = X.iloc[:n], y.iloc[:n]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining dataset and dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DigitsDataset(Dataset):\n",
    "    def __init__(self, X: pd.DataFrame, y: pd.Series, edge: int = 28):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.edge = edge\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "\n",
    "    def __getitem__(self, idx: int) -> T.Tuple[torch.Tensor, int]:\n",
    "        img = (\n",
    "            torch.from_numpy(self.X.iloc[idx].values / 255.0)  # normalizing\n",
    "            .reshape(self.edge, self.edge)\n",
    "            .double()\n",
    "        )\n",
    "        label = int(self.y.iloc[idx])\n",
    "        return (img, label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = DigitsDataset(X0, y0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item = ds[4]\n",
    "plt.imshow(item[0], cmap=\"gray\", origin=\"upper\")\n",
    "plt.title(f\"Label: {item[1]}\")\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1\n",
    "dataloader = DataLoader(ds, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features, train_labels = next(iter(dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Feature batch shape: {train_features.size()}\")\n",
    "print(f\"Labels batch shape: {train_labels.size()}\")\n",
    "img = train_features[0]  # .reshape((28,28))\n",
    "label = train_labels[0]\n",
    "plt.imshow(img, cmap=\"gray\")\n",
    "plt.show()\n",
    "print(f\"Label: {label}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_conv_output_dim(input_dim, kernel_size, padding, stride):\n",
    "    return int((input_dim - kernel_size + 2 * padding) / stride + 1)\n",
    "\n",
    "\n",
    "calc_conv_output_dim(28, 5, 2, 2), calc_conv_output_dim(14, 5, 2, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `MyConv2d`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyConv2d(torch.nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        edge: int,\n",
    "        n_in_channels: int = 1,\n",
    "        n_out_channels: int = 1,\n",
    "        kernel_width: int = 5,\n",
    "        kernel_height: int = 5,\n",
    "        stride: int = 1,\n",
    "        padding: int = 0,\n",
    "        dilation: int = 1,\n",
    "        lecun_init: bool = True,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.register_buffer(\"edge\", torch.tensor(edge))\n",
    "        self.register_buffer(\"n_in_channels\", torch.tensor(n_in_channels))\n",
    "        self.register_buffer(\"n_out_channels\", torch.tensor(n_out_channels))\n",
    "        self.register_buffer(\"kernel_width\", torch.tensor(kernel_width))\n",
    "        self.register_buffer(\"kernel_height\", torch.tensor(kernel_height))\n",
    "        self.register_buffer(\"stride\", torch.tensor(stride))\n",
    "        self.register_buffer(\"padding\", torch.tensor(padding))\n",
    "        self.register_buffer(\"dilation\", torch.tensor(dilation))\n",
    "\n",
    "        self.weight = nn.Parameter(\n",
    "            torch.empty(\n",
    "                n_in_channels * kernel_width * kernel_height,\n",
    "                n_out_channels,\n",
    "                dtype=torch.double,\n",
    "            )\n",
    "        )\n",
    "        self.bias = nn.Parameter(\n",
    "            torch.empty(1, n_out_channels, 1, 1, dtype=torch.double)\n",
    "        )\n",
    "\n",
    "        # self.bias = rearrange(self.bias, \"out_channels -> 1 out_channels 1 1\")\n",
    "\n",
    "        if lecun_init:\n",
    "            s = 2.4 / (n_in_channels * kernel_width * kernel_height)\n",
    "            self.weight.data.uniform_(-s, s)\n",
    "            self.bias.data.uniform_(-s, s)\n",
    "\n",
    "        else:\n",
    "            self.weight.data.normal_(0, 1.0)\n",
    "            self.bias.data.normal_(0, 1.0)\n",
    "\n",
    "        self.unfold = torch.nn.Unfold(\n",
    "            kernel_size=(kernel_height, kernel_width),\n",
    "            dilation=dilation,\n",
    "            padding=padding,\n",
    "            stride=stride,\n",
    "        )\n",
    "        out_h = out_w = calc_conv_output_dim(\n",
    "            edge, kernel_width, padding, stride\n",
    "        )\n",
    "        self.fold = torch.nn.Fold(\n",
    "            output_size=(out_h, out_w),\n",
    "            kernel_size=(1, 1),\n",
    "            dilation=dilation,\n",
    "            padding=0,\n",
    "            stride=1,\n",
    "        )\n",
    "\n",
    "    def forward(self, input: torch.Tensor) -> torch.Tensor:\n",
    "        # inspired by: https://discuss.pytorch.org/t/make-custom-conv2d-layer-efficient-wrt-speed-and-memory/70175/2\n",
    "\n",
    "        # (N,C,in_h,in_w) -> (N, C*kh*kw, num_patches)\n",
    "        # N = batch_size, C = in_channels, kh = kernel_height, kw = kernel_width\n",
    "        input_unfolded = self.unfold(input)\n",
    "\n",
    "        input_unfolded = rearrange(\n",
    "            input_unfolded, \"N r num_patches -> N num_patches r\"\n",
    "        )\n",
    "\n",
    "        output_unfolded = input_unfolded @ self.weight\n",
    "        output_unfolded = rearrange(\n",
    "            output_unfolded,\n",
    "            \"N num_patches out_channels -> N out_channels num_patches\",\n",
    "        )\n",
    "\n",
    "        output = self.fold(output_unfolded)  # (N, out_channels, out_h, out_w)\n",
    "        if self.bias is not None:\n",
    "            output += self.bias\n",
    "\n",
    "        return output\n",
    "\n",
    "\n",
    "kh = kw = 5\n",
    "n_in_channels = 1\n",
    "n_out_channels = 1\n",
    "weight = torch.randn(n_out_channels, n_in_channels, kw, kw).double()\n",
    "bias = torch.randn(n_out_channels).double()\n",
    "\n",
    "print(f\"{weight.shape=}\")\n",
    "\n",
    "train_features, train_labels = next(iter(dataloader))\n",
    "train_features = train_features.unsqueeze(dim=1)\n",
    "\n",
    "print(f\"{train_features.shape=}\")\n",
    "\n",
    "myconv2d = MyConv2d(\n",
    "    edge=28,\n",
    "    n_in_channels=n_in_channels,\n",
    "    n_out_channels=n_out_channels,\n",
    "    kernel_width=kw,\n",
    "    kernel_height=kh,\n",
    "    stride=2,\n",
    "    padding=2,\n",
    "    dilation=1,\n",
    ")\n",
    "conv_features = myconv2d(train_features)\n",
    "\n",
    "print(f\"{conv_features.shape=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = train_labels[0]\n",
    "fig, axs = plt.subplots(nrows=2, ncols=1, figsize=(5, 5))\n",
    "\n",
    "ax = axs[0]\n",
    "img = train_features[0][0]\n",
    "ax.imshow(img, cmap=\"gray\")\n",
    "\n",
    "ax = axs[1]\n",
    "img = conv_features.detach().numpy()[0][0]\n",
    "ax.imshow(img, cmap=\"gray\")\n",
    "\n",
    "plt.show()\n",
    "print(f\"Label: {label}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### just trying to overfit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ParameterHistory:\n",
    "    def __init__(self, every_n: int = 1):\n",
    "        self.history = defaultdict(list)\n",
    "        self.every_n = every_n\n",
    "        self.iter = []\n",
    "\n",
    "    def __call__(self, model: nn.Module, _iter: int):\n",
    "        if _iter % self.every_n != 0:\n",
    "            return\n",
    "        state_dict = model.state_dict()\n",
    "\n",
    "        for name, tensor in state_dict.items():\n",
    "            self.history[name].append(tensor.clone().numpy().ravel())\n",
    "\n",
    "        self.iter.append(_iter)\n",
    "\n",
    "    def get_df(self, name: str) -> pd.DataFrame:\n",
    "        df = [\n",
    "            pd.DataFrame({\"value\": w}).assign(iter=i)\n",
    "            for i, w in zip(self.iter, self.history[name])\n",
    "        ]\n",
    "        return pd.concat(df, ignore_index=True)[[\"iter\", \"value\"]]\n",
    "\n",
    "    def get_rolling_mean_df(self, name: str, window: int = 10) -> pd.DataFrame:\n",
    "        df = self.get_df(name)\n",
    "        df_roll = df.rolling(window=window, on=\"iter\", min_periods=1).mean()\n",
    "        if \"iter\" not in df_roll.columns:\n",
    "            df_roll[\"iter\"] = range(len(df_roll))\n",
    "        return df_roll\n",
    "\n",
    "\n",
    "class LossHistory:\n",
    "    def __init__(self, every_n: int = 1):\n",
    "        self.history = []\n",
    "        self.iter = []\n",
    "        self.every_n = every_n\n",
    "\n",
    "    def __call__(self, loss: torch.Tensor, _iter: int):\n",
    "        if _iter % self.every_n != 0:\n",
    "            return\n",
    "        self.history.append(loss.item())\n",
    "        self.iter.append(_iter)\n",
    "\n",
    "    def get_df(self) -> pd.DataFrame:\n",
    "        return pd.DataFrame({\"iter\": self.iter, \"loss\": self.history})\n",
    "\n",
    "    def get_rolling_mean_df(self, window: int = 10) -> pd.DataFrame:\n",
    "        df = self.get_df()\n",
    "        df_roll = df.rolling(window=window, on=\"iter\", min_periods=1).mean()\n",
    "        if \"iter\" not in df_roll.columns:\n",
    "            df_roll[\"iter\"] = range(len(df_roll))\n",
    "        return df_roll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def densify_y(y: torch.Tensor) -> torch.Tensor:\n",
    "    new_y = F.one_hot(y, num_classes=10)\n",
    "    new_y[new_y == 0] = -1\n",
    "    return new_y.double()\n",
    "\n",
    "\n",
    "train_labels[0:3], densify_y(train_labels[0:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TanhLeCun1990(nn.Module):\n",
    "    def __init__(self, A: float = 1.716, S: float = 2 / 3):\n",
    "        super().__init__()\n",
    "        self.A = A\n",
    "        self.S = S\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        return self.A * torch.tanh(self.S * x)\n",
    "\n",
    "\n",
    "class Model(nn.Module):\n",
    "    # inspired by https://einops.rocks/pytorch-examples.html\n",
    "    def __init__(\n",
    "        self,\n",
    "        edge: int = 28,\n",
    "        n_classes: int = 10,\n",
    "        lecun_init: bool = True,\n",
    "        lecun_act: bool = True,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        # self.conv1 = nn.Conv2d(1, 12, kernel_size=5, stride=2, padding=2)\n",
    "        self.conv1 = MyConv2d(\n",
    "            edge=edge,\n",
    "            n_in_channels=1,\n",
    "            n_out_channels=12,\n",
    "            kernel_width=5,\n",
    "            kernel_height=5,\n",
    "            stride=2,\n",
    "            padding=2,\n",
    "            lecun_init=lecun_init,\n",
    "        )\n",
    "        edge = edge // 2  # effect of stride\n",
    "        # self.conv2 = nn.Conv2d(12, 12, kernel_size=5, stride=2, padding=2)\n",
    "        self.conv2 = MyConv2d(\n",
    "            edge=edge,\n",
    "            n_in_channels=12,\n",
    "            n_out_channels=12,\n",
    "            kernel_width=5,\n",
    "            kernel_height=5,\n",
    "            stride=2,\n",
    "            padding=2,\n",
    "            lecun_init=lecun_init,\n",
    "        )\n",
    "        edge = edge // 2  # effect of stride\n",
    "        self.lin1 = nn.Linear(edge * edge * 12, 30)\n",
    "        self.lin2 = nn.Linear(30, n_classes)\n",
    "\n",
    "        if lecun_init:\n",
    "            s = 2.4 / self.lin1.weight.shape[0]\n",
    "            self.lin1.weight.data.uniform_(-s, s)\n",
    "\n",
    "            s = 2.4 / self.lin2.weight.shape[0]\n",
    "            self.lin2.weight.data.uniform_(-s, s)\n",
    "\n",
    "        if lecun_act:\n",
    "            self.act = TanhLeCun1990()\n",
    "        else:\n",
    "            self.act = F.tanh\n",
    "\n",
    "        self.net = nn.Sequential(\n",
    "            Rearrange(\"b h w -> b 1 h w\"),\n",
    "            self.conv1,\n",
    "            self.act,\n",
    "            self.conv2,\n",
    "            self.act,\n",
    "            Rearrange(\"b c h w -> b (c h w)\"),\n",
    "            self.lin1,\n",
    "            self.act,\n",
    "            self.lin2,\n",
    "            self.act,\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(lecun_init=True, lecun_act=True)\n",
    "model.double()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = SGD(\n",
    "    model.parameters(),\n",
    "    lr=0.1,  # randomly chosen, not provided in the paper\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss_func = nn.CrossEntropyLoss()\n",
    "loss_func = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.to(device);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameter_history = ParameterHistory(every_n=100)\n",
    "loss_history = LossHistory(every_n=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 20\n",
    "_iter = 0\n",
    "model.train()\n",
    "for epoch in tqdm.tqdm(range(n_epochs), desc=\"Epochs\", total=n_epochs):\n",
    "    for i, (xb, yb) in tqdm.tqdm(\n",
    "        enumerate(dataloader), desc=\"Batches\", total=len(dataloader)\n",
    "    ):\n",
    "        xb = xb.to(device)\n",
    "        yb = yb.to(device)\n",
    "        yb = densify_y(yb)\n",
    "        loss = loss_func(model(xb), yb)\n",
    "\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "\n",
    "        parameter_history(model, _iter)\n",
    "        loss_history(loss, _iter)\n",
    "\n",
    "        _iter += 1\n",
    "\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(12, 4), sharex=True)\n",
    "\n",
    "df = loss_history.get_df()\n",
    "df_roll = loss_history.get_rolling_mean_df(window=10)\n",
    "\n",
    "sns.lineplot(data=df, x=\"iter\", y=\"loss\", ax=ax, label=\"Train\")\n",
    "sns.lineplot(\n",
    "    data=df_roll, x=\"iter\", y=\"loss\", ax=ax, label=\"Train (rolling mean)\"\n",
    ")\n",
    "ax.set(xlabel=\"Iter\", ylabel=\"Loss\", title=\"Loss History\")\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "display(df_roll.tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_history(\n",
    "    history: ParameterHistory,\n",
    "    name: str,\n",
    "    figsize: T.Tuple[int, int] = (12, 4),\n",
    "    weight_bins: int = 20,\n",
    "    bias_bins: int = 10,\n",
    ") -> None:\n",
    "    fig, axs = plt.subplots(figsize=figsize, nrows=2, sharex=True)\n",
    "\n",
    "    ax = axs[0]\n",
    "    _name = f\"{name}.weight\"\n",
    "    df = history.get_df(_name)\n",
    "\n",
    "    n_iter = df[\"iter\"].nunique()\n",
    "    bins = (n_iter, weight_bins)\n",
    "    sns.histplot(\n",
    "        data=df,\n",
    "        x=\"iter\",\n",
    "        y=\"value\",\n",
    "        ax=ax,\n",
    "        thresh=None,\n",
    "        cmap=\"plasma\",\n",
    "        bins=bins,\n",
    "    )\n",
    "    ax.set_ylabel(\"weight\")\n",
    "    ax.set_title(name)\n",
    "\n",
    "    ax = axs[1]\n",
    "    _name = f\"{name}.bias\"\n",
    "    df = history.get_df(_name)\n",
    "\n",
    "    bins = (n_iter, bias_bins)\n",
    "    sns.histplot(\n",
    "        data=df,\n",
    "        x=\"iter\",\n",
    "        y=\"value\",\n",
    "        ax=ax,\n",
    "        thresh=None,\n",
    "        cmap=\"plasma\",\n",
    "        bins=bins,\n",
    "    )\n",
    "    ax.set_xlabel(\"iter\")\n",
    "    ax.set_ylabel(\"bias\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "draw_history(parameter_history, \"conv1\")\n",
    "draw_history(parameter_history, \"conv2\")\n",
    "draw_history(parameter_history, \"lin1\", weight_bins=100)\n",
    "draw_history(parameter_history, \"lin2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features, train_labels = next(iter(dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features = train_features.to(device)\n",
    "pred_probs = model(train_features)\n",
    "pred_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = pred_probs.to(\"cpu\").detach().numpy().argmax(axis=1)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Feature batch shape: {train_features.size()}\")\n",
    "print(f\"Labels batch shape: {train_labels.size()}\")\n",
    "img = train_features[0].cpu()  # .reshape((28,28))\n",
    "label = train_labels[0]\n",
    "plt.imshow(img, cmap=\"gray\")\n",
    "plt.show()\n",
    "print(f\"Label: {label}, pred: {y_pred[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(model.conv1.buffers())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_filters = model.conv1.n_out_channels.item()\n",
    "\n",
    "fig, axs = plt.subplots(\n",
    "    nrows=n_filters // 3, ncols=n_filters // 4, figsize=(12, 12)\n",
    ")\n",
    "with torch.no_grad():\n",
    "    conv_features = model.act(model.conv1(train_features.unsqueeze(1)))\n",
    "    for i, ax in enumerate(axs.flatten()):\n",
    "        ax.imshow(conv_features[0][i], cmap=\"gray\")\n",
    "        ax.axis(\"off\")\n",
    "        ax.set_title(f\"Filter {i+1}\")\n",
    "\n",
    "plt.show()\n",
    "print(f\"Label: {label}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting decent performance on 80:20 split over 10k digits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 10_000  # bit larger than the number of samples in the paper\n",
    "X0, X1, y0, y1 = train_test_split(\n",
    "    X.iloc[:n], y.iloc[:n], test_size=0.2, random_state=42\n",
    ")  # , stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = DigitsDataset(X0, y0)\n",
    "ds_test = DigitsDataset(X1, y1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1\n",
    "dataloader = DataLoader(ds, batch_size=batch_size, shuffle=False)\n",
    "dataloader_test = DataLoader(ds_test, batch_size=500, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(lecun_init=True, lecun_act=True)\n",
    "model.double()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = SGD(\n",
    "    model.parameters(),\n",
    "    lr=0.01,  # randomly chosen, not provided in the paper\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss_func = nn.CrossEntropyLoss()\n",
    "loss_func = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.to(device);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameter_history = ParameterHistory(every_n=100)\n",
    "loss_history = LossHistory(every_n=100)\n",
    "loss_history_test = LossHistory(every_n=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 10\n",
    "_iter = 0\n",
    "model.train()\n",
    "for epoch in tqdm.tqdm(range(n_epochs), desc=\"Epochs\", total=n_epochs):\n",
    "    for i, (xb, yb) in tqdm.tqdm(\n",
    "        enumerate(dataloader), desc=\"Batches\", total=len(dataloader)\n",
    "    ):\n",
    "        xb = xb.to(device)\n",
    "        yb = yb.to(device)\n",
    "        yb = densify_y(yb)\n",
    "        loss = loss_func(model(xb), yb)\n",
    "\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "\n",
    "        parameter_history(model, _iter)\n",
    "        loss_history(loss, _iter)\n",
    "\n",
    "        _iter += 1\n",
    "\n",
    "    # compute validation loss\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        ys_pred, ys_true = [], []\n",
    "        for xb, yb in dataloader_test:\n",
    "            xb = xb.to(device)\n",
    "            yb = yb.to(device)\n",
    "            yb = densify_y(yb)\n",
    "            yp = model(xb)\n",
    "            ys_pred.append(yp)\n",
    "            ys_true.append(yb)\n",
    "        y_pred = torch.cat(ys_pred, dim=0)\n",
    "        y_true = torch.cat(ys_true, dim=0)\n",
    "        loss_test = loss_func(y_pred, y_true)\n",
    "        loss_history_test(loss_test, _iter)\n",
    "        model.train()\n",
    "\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(nrows=2, ncols=1, figsize=(12, 4), sharex=True)\n",
    "\n",
    "ax = axs[0]\n",
    "df = loss_history.get_df()\n",
    "df_roll = loss_history.get_rolling_mean_df(window=10)\n",
    "\n",
    "sns.lineplot(data=df, x=\"iter\", y=\"loss\", ax=ax, label=\"Train\")\n",
    "sns.lineplot(\n",
    "    data=df_roll, x=\"iter\", y=\"loss\", ax=ax, label=\"Train (rolling mean)\"\n",
    ")\n",
    "ax.set(xlabel=\"Iter\", ylabel=\"Loss\", title=\"Loss History\")\n",
    "\n",
    "ax = axs[1]\n",
    "df = loss_history_test.get_df()\n",
    "df_roll = loss_history_test.get_rolling_mean_df(window=10)\n",
    "\n",
    "sns.lineplot(data=df, x=\"iter\", y=\"loss\", ax=ax, label=\"Test\")\n",
    "sns.lineplot(\n",
    "    data=df_roll, x=\"iter\", y=\"loss\", ax=ax, label=\"Test (rolling mean)\"\n",
    ")\n",
    "ax.set(xlabel=\"Iter\", ylabel=\"Loss\", title=\"Test Loss History\")\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "display(df_roll.tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_history(parameter_history, \"conv1\")\n",
    "draw_history(parameter_history, \"conv2\")\n",
    "draw_history(parameter_history, \"lin1\", weight_bins=100)\n",
    "draw_history(parameter_history, \"lin2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features, train_labels = next(iter(dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features = train_features.to(device)\n",
    "pred_probs = model(train_features)\n",
    "pred_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = pred_probs.to(\"cpu\").detach().numpy().argmax(axis=1)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Feature batch shape: {train_features.size()}\")\n",
    "print(f\"Labels batch shape: {train_labels.size()}\")\n",
    "img = train_features[0].cpu()  # .reshape((28,28))\n",
    "label = train_labels[0]\n",
    "plt.imshow(img, cmap=\"gray\")\n",
    "plt.show()\n",
    "print(f\"Label: {label}, pred: {y_pred[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_filters = model.conv1.n_out_channels.item()\n",
    "\n",
    "fig, axs = plt.subplots(\n",
    "    nrows=n_filters // 3, ncols=n_filters // 4, figsize=(12, 12)\n",
    ")\n",
    "with torch.no_grad():\n",
    "    conv_features = model.act(model.conv1(train_features.unsqueeze(1)))\n",
    "    for i, ax in enumerate(axs.flatten()):\n",
    "        ax.imshow(conv_features[0][i], cmap=\"gray\")\n",
    "        ax.axis(\"off\")\n",
    "        ax.set_title(f\"Filter {i+1}\")\n",
    "\n",
    "plt.show()\n",
    "print(f\"Label: {label}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ys_pred = []\n",
    "ys_true = []\n",
    "for test_features, test_labels in dataloader_test:\n",
    "    test_features = test_features.to(device)\n",
    "    pred_probs = model(test_features)\n",
    "\n",
    "    y_pred = pred_probs.to(\"cpu\").detach().numpy().argmax(axis=1)\n",
    "\n",
    "    ys_true.append(test_labels.numpy())\n",
    "    ys_pred.append(y_pred)\n",
    "\n",
    "ys_true = np.concatenate(ys_true)\n",
    "ys_pred = np.concatenate(ys_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ys_true, ys_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = metrics.accuracy_score(ys_true, ys_pred)\n",
    "error_rate = 1 - accuracy\n",
    "print(f\"Accuracy: {accuracy:.2%}\")\n",
    "print(f\"Error rate: {error_rate:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = metrics.ConfusionMatrixDisplay.from_predictions(ys_true, ys_pred)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
